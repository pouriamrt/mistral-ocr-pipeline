
# ğŸ§  Mistral OCR Annotation Pipeline
[![Python](https://img.shields.io/badge/python-3.13%2B-blue.svg)](https://www.python.org/) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](#-license) 
[![Build](https://img.shields.io/badge/status-async%20pipeline-success)](#-quickstart)

A **highâ€‘throughput, asynchronous** pipeline for **OCR-driven extraction, annotation, and markdown generation** from scientific PDFs using **Mistral OCR**.  
Outputs include **annotated Markdown** files and an **aggregated CSV/Parquet** of structured fields defined by a robust **Pydantic schema**.

---

## âœ¨ Highlights
- âš¡ **Asynchronous batching** of PDFs with configurable concurrency and rate limiting
- ğŸ§© **Schema-driven extraction** via `ExtractionPayload` (Pydantic) with multiple extraction classes
- ğŸ“ **Markdown reports** with optional image annotations (base64 inlined)
- ğŸ“Š **Aggregated outputs** in CSV and Parquet for downstream analysis
- ğŸ›¡ï¸ **Graceful error handling** with per-chunk resilience and retry logic
- ğŸ” **Post-processing validation** with LLM-based field verification
- ğŸ“ **Logging** to `logs/pipeline.log` with `loguru` and rich console output
- ğŸ”„ **Resume capability** to skip already-processed PDFs

---

## ğŸ—‚ï¸ Project Structure
> The repository ships with a modular, package-based layout. At runtime the pipeline creates output folders.

```
.
â”œâ”€ info_extraction/             # Core extraction package
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ extraction_payload.py    # Pydantic schema capturing all target fields
â”‚  â”œâ”€ get_annotations.py        # Mistral OCR client + async wrapper with rate limiting
â”‚  â””â”€ to_markdown.py            # Transforms OCR response to Markdown with image annotations
â”œâ”€ utils/                       # Utility functions package
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ utils.py                  # Async I/O utilities: base64 encode, page count, dict merging
â”‚  â””â”€ diagram.py                # Flow diagram generation for pipeline visualization
â”œâ”€ post_processing/             # Post-processing and validation package
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ post_processing.py        # LLM-based field validation and quality checks
â”‚  â””â”€ unstack_payloads.py       # Field configuration and payload unstacking utilities
â”œâ”€ main.py                      # Orchestrates concurrency, aggregation, and persistence
â”œâ”€ pyproject.toml               # Project metadata / dependencies (for uv/pip)
â”œâ”€ uv.lock                      # (uv) resolved dependency lock
â”œâ”€ .env.example                 # Example environment variables (copy to .env)
â”œâ”€ papers/                      # (create) input PDFs to process
â”‚  â””â”€ your_paper_1.pdf
â”‚  â””â”€ your_paper_2.pdf
â”œâ”€ output/                      # (auto-created) per-chunk Markdown exports
â”‚  â”œâ”€ <paper_stem>_0.md
â”‚  â”œâ”€ <paper_stem>_1.md
â”‚  â””â”€ aggregated/               # (auto-created) final tabular outputs
â”‚     â”œâ”€ df_annotations.csv
â”‚     â””â”€ df_annotations.parquet
â”œâ”€ logs/                        # (auto-created) logs
â”‚  â””â”€ pipeline.log
â”œâ”€ data/                        # (optional) Additional data files for analysis
â””â”€ README.md
```

---

## ğŸ§© Data Model (ExtractionPayload)
The pipeline extracts a rich set of biomedical/experimental fields (journal, design, cohorts, assay methods, timing, thresholds, outcomes, etc.) encoded in `extraction_payload.py`.  
Use these fields directly in analytics or dashboards (e.g., study design distribution, assay performance summaries).

> **Tip:** You can extend the schema at any timeâ€”new fields flow through to CSV/Parquet automatically.

---

## âš™ï¸ Setup

### 1) Clone
```bash
git clone https://github.com/yourname/mistral-ocr-pipeline.git
cd mistral-ocr-pipeline
```

### 2) Python & Dependencies
**Requires Python 3.13+**

Using `uv` (recommended):
```bash
uv sync
```

Or using `pip`:
```bash
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -e .
```

**Key dependencies:**
- `mistralai` - Mistral OCR API client
- `pydantic` - Schema validation
- `aiofiles` - Async file I/O
- `pypdf` - PDF metadata extraction
- `pandas` / `pyarrow` - Data aggregation
- `langchain` / `langchain-openai` - Post-processing validation
- `loguru` - Structured logging
- `winloop` / `uvloop` - High-performance async event loop

### 3) Environment
Create a `.env` from the example and add your key:
```bash
cp .env.example .env
# then edit .env
MISTRAL_API_KEY=your_api_key_here
MAX_CONCURRENCY=3
IMAGE_ANNOTATION=False
OVERWRITE_MD=True
MODEL_JUDGE=gpt-5-mini  # For post-processing validation (optional)
```

### 4) Input PDFs
Place files in `./papers/`. The pipeline will scan `*.pdf` automatically.

---

## ğŸš€ Quickstart
```bash
python main.py
```

**What youâ€™ll get**
- `output/<paper>_*.md` â€” readable per-chunk Markdown (includes document annotation and inlined images if enabled)
- `output/aggregated/df_annotations.csv` â€” one row per processed PDF with structured fields
- `output/aggregated/df_annotations.parquet` â€” fast, columnar equivalent (if `pyarrow` is installed)

---

## ğŸ§  How It Works
1. **Load & Encode** â€” `utils.utils.encode_pdf` base64-encodes PDFs asynchronously.  
2. **OCR + Annotation** â€” `info_extraction.get_annotations` calls Mistral OCR with rate limiting and retry logic, using **document annotation format** mapped to `ExtractionPayload` classes.  
3. **Chunking** â€” Large PDFs are processed in **MAX_PAGES_PER_REQ** chunks with **async concurrency** and semaphore-based rate limiting.  
4. **Markdown** â€” `info_extraction.to_markdown` builds consolidated Markdown with (optional) image annotations.  
5. **Aggregation** â€” Partial rows from chunks are **deduped/merged** using `merge_multiple_dicts_async` and written to CSV/Parquet.  
6. **Post-processing** (optional) â€” `post_processing.post_processing` provides LLM-based validation of extracted fields.

**Ascii flow:**  
```
PDFs -> base64 -> Mistral OCR (rate-limited) -> JSON (ExtractionPayload) -> Markdown + Tabular -> CSV/Parquet
                                                                                â†“
                                                                      Post-processing (validation)
```

---

## ğŸ”§ Configuration
Tune behavior via `.env`:
```ini
MISTRAL_API_KEY=...     # required
MAX_CONCURRENCY=3       # concurrent OCR calls
IMAGE_ANNOTATION=False  # enable base64 image inlining in markdown
OVERWRITE_MD=True       # overwrite existing markdown files
MODEL_JUDGE=gpt-5-mini  # LLM model for post-processing validation
```

Edit constants in `main.py` for chunk sizing:
```python
MAX_PAGES_PER_REQ = 8  # pages per OCR request
```

The pipeline uses `winloop` (Windows) or `uvloop` (Unix) for high-performance async I/O.

---

## ğŸ“˜ Usage Notes
- For **very long PDFs**, results are merged across chunks using `merge_multiple_dicts_async`.  
- To **enable image annotations**, set `IMAGE_ANNOTATION=True` in `.env` or pass `image_annotation=True` to the processing function.  
- Parquet output requires `pyarrow` (included in dependencies).  
- The pipeline supports **resume mode**: if `OVERWRITE_MD=False`, already-processed PDFs (tracked by SHA1 hash) are skipped.  
- Rate limiting is built into the OCR client to respect API limits (configurable via `OCR_RPS`).

---

## ğŸ§ª Testing Locally
```bash
# Dry-run with a single sample
python -c "from pathlib import Path; print(list(Path('papers').glob('*.pdf'))[:1])"
python main.py
```

---

## ğŸ› ï¸ Troubleshooting
- **`MISTRAL_API_KEY is not set`** â†’ Ensure `.env` is present and loaded, or export variable in shell.  
- **Parquet save failed** â†’ Install `pyarrow`: `pip install pyarrow`.  
- **No PDFs found** â†’ Confirm files exist in `./papers/` and match `*.pdf`.  
- **Rate limits / timeouts** â†’ Lower `MAX_CONCURRENCY` or `MAX_PAGES_PER_REQ`.

---

## ğŸ—ºï¸ Roadmap

Weâ€™re continuously evolving to make your experience better! Here are some highlights:

- âœ… Reliable: Automatic retry & backoff for transient OCR errors using `tenacity`
- âœ… Smooth operation: Built-in rate limiting for safe and efficient OCR API usage
- âœ… Robust: Effortless resume for already-processed PDFsâ€”no lost progress
- âœ… Quality assurance: Integrated LLM-powered post-processing validation

**Coming soon to make your workflow even more seamless:**
- ğŸš€ Flexible CLI flags (customize input/output directories, select page ranges)
- ğŸŒˆ Beautiful rich HTML reports with embedded assets
- ğŸ“Š Enhanced batch processing with intuitive progress bars

Have feedback or ideas? Weâ€™d love to hear from you as we continue to build!

---

## ğŸ™Œ Acknowledgments
- **Mistral AI** â€” OCR + annotation interfaces  
- **Pydantic** â€” robust schema modeling  
- **pandas / pyarrow** â€” analytics-ready outputs  
- **pypdf** â€” fast PDF metadata access
- **loguru** â€” structured logging
- **langchain** â€” LLM integration for post-processing
- **winloop / uvloop** â€” high-performance async event loops
- **tenacity** â€” retry logic with exponential backoff

---

## ğŸªª License
MIT Â© 2025 Pouria Mortezaagha
